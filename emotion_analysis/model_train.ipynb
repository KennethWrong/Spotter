{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f10b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import neattext.functions as nfx\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3592e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data import get_tokenizer\n",
    "from collections import Counter\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f0f41cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09a64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None, dtype=\"float32\"):\n",
    "    y = np.array(y, dtype=\"int\")\n",
    "    input_shape = y.shape\n",
    "\n",
    "    # Shrink the last dimension if the shape is (..., 1).\n",
    "    if input_shape and input_shape[-1] == 1 and len(input_shape) > 1:\n",
    "        input_shape = tuple(input_shape[:-1])\n",
    "\n",
    "    y = y.reshape(-1)\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes), dtype=dtype)\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    output_shape = input_shape + (num_classes,)\n",
    "    categorical = np.reshape(categorical, output_shape)\n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "142acce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_matrix, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        embedding_dim = embedding_matrix.size(1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=embedding_matrix)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x) # [1, 79, 200]\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden) # [1, 79, 128]\n",
    "        print(hidden)\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)  #[79, 128]\n",
    "    \n",
    "        \n",
    "        out = self.dropout(lstm_out) # [79, 128]\n",
    "        out = self.fc(out) # [79, 8]\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]  \n",
    "        out = self.Softmax(out)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(torch.float32).to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(torch.float32).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "85cccb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = pd.read_csv(\"../data/data.csv\")\n",
    "text_labels = pd.read_csv(\"../data/labels.csv\")\n",
    "\n",
    "label_emotion = text_labels['Emotion'].tolist()\n",
    "label_encoded = text_labels['emotion_encoded'].tolist()\n",
    "\n",
    "label_mapping = {idx: emotion for idx,emotion in zip(label_encoded, label_emotion)}\n",
    "vectorized_labels = to_categorical(label_encoded)\n",
    "\n",
    "sentences = text_data['Clean_Text'].tolist()\n",
    "full_doc = \" \".join([str(s) for s in sentences])\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokenized_doc = tokenizer(full_doc)\n",
    "\n",
    "vocab = list(set(tokenized_doc))\n",
    "\n",
    "tokens_map_wn = {token:idx+1 for idx,token in enumerate(vocab)}\n",
    "tokens_map_nw = {idx+1:token for idx,token in enumerate(vocab)}\n",
    "tokens_map_nw[0] = \"\"\n",
    "tokens_map_wn[\"\"] = 0\n",
    "\n",
    "\n",
    "\n",
    "sentence_tokenized = [tokenizer(str(s)) for s in sentences]\n",
    "sentence_tokenized = [[tokens_map_wn[w] for w in s] for s in sentence_tokenized]\n",
    "\n",
    "max_sequence_length = max([len(s) for s in sentence_tokenized])\n",
    "vocab_size = len(vocab) + 1\n",
    "\n",
    "sentence_tokenized_p = pad_sequences(sentence_tokenized, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "sentence_tokenized_p = np.array(sentence_tokenized_p)\n",
    "\n",
    "glove_file = \"glove.6B.200d.txt\"\n",
    "glove_path = \"../data/glove/\" + glove_file\n",
    "\n",
    "glove = {}\n",
    "\n",
    "with open(glove_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        glove[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, 200), dtype=\"float32\")\n",
    "\n",
    "\n",
    "for token in tokenized_doc:\n",
    "    embedding_vector = glove.get(token, None)\n",
    "\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[tokens_map_wn[token]] = embedding_vector\n",
    "\n",
    "\n",
    "sentence_embedded_p = np.array([[embedding_matrix[t,:] for t in q] for q in sentence_tokenized_p.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dddb4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_labels_t = torch.Tensor(vectorized_labels, device=device)\n",
    "embedding_matrix_t = torch.Tensor(embedding_matrix, device=device).to(torch.float32)\n",
    "training_data = torch.Tensor(sentence_tokenized_p, device=device).to(torch.long)\n",
    "\n",
    "train_X, sec_X, train_y, sec_y = train_test_split(training_data, vectorized_labels_t, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8cc7d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_balance_weight(y):\n",
    "    y = torch.argmax(y, dim=1).tolist()\n",
    "    count = Counter(y)\n",
    "    count = sorted([(k,v) for k,v in count.items()])\n",
    "    count = np.array([c[1] for c in count])\n",
    "\n",
    "    return torch.Tensor(1. / count, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "94f798b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionModel(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_matrix, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(EmotionModel, self).__init__()\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        embedding_dim = embedding_matrix.size(1)\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, _weight=embedding_matrix)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.Softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        batch_size = x.size(0)\n",
    "        embeds = self.embedding(x) # [1, 79, 200]\n",
    "        lstm_out, (hidden, cell) = self.lstm(embeds, hidden) # [1, 79, 128]\n",
    "\n",
    "        # hid = self.dropout(hidden[-1,:,:])\n",
    "        # out = self.sigmoid(self.fc(hid))\n",
    "\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)  #[79, 128]\n",
    "    \n",
    "        \n",
    "        out = self.dropout(lstm_out) # [79, 128]\n",
    "        out = self.fc(out) # [79, 8]\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        out = out.view(batch_size, -1, self.output_size)\n",
    "        out = out[:, -1]  \n",
    "        \n",
    "        return out, (hidden, cell)\n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(torch.float32).to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(torch.float32).to(device))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "83a1d328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1:\n",
      "tensor([[0.5072, 0.4864, 0.4894, 0.4942, 0.4888, 0.5077, 0.5128, 0.4935],\n",
      "        [0.5123, 0.4914, 0.4930, 0.4962, 0.4839, 0.5034, 0.5182, 0.4925],\n",
      "        [0.5009, 0.4856, 0.4876, 0.5003, 0.4905, 0.5079, 0.5219, 0.5016],\n",
      "        [0.5131, 0.4941, 0.4908, 0.4972, 0.4896, 0.5100, 0.5236, 0.5016]],\n",
      "       grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 0.6884104609489441\n",
      "tensor([[2.3613e-02, 1.3158e-03, 2.0632e-01, 5.4932e-01, 1.2896e-03, 9.2684e-04,\n",
      "         1.3130e-01, 1.2637e-03],\n",
      "        [9.9669e-01, 1.8401e-05, 9.6273e-04, 5.0485e-04, 1.9426e-05, 2.0172e-05,\n",
      "         1.7791e-03, 1.9387e-05],\n",
      "        [9.9532e-01, 1.1284e-05, 6.0532e-04, 5.9717e-04, 1.7778e-05, 7.2224e-06,\n",
      "         4.9349e-03, 1.7286e-05],\n",
      "        [9.9898e-01, 1.0002e-05, 6.2196e-04, 2.5306e-04, 1.4108e-05, 8.3874e-06,\n",
      "         1.0213e-03, 1.0062e-05]], grad_fn=<SelectBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 0.03187786415219307\n",
      "tensor([[1.1840e-02, 3.1256e-04, 9.7839e-03, 9.6862e-01, 2.7557e-04, 1.9992e-04,\n",
      "         1.3365e-02, 2.3546e-04],\n",
      "        [9.9944e-01, 5.3650e-06, 3.9038e-04, 2.0988e-04, 4.6338e-06, 7.4807e-06,\n",
      "         1.7748e-04, 4.3098e-06],\n",
      "        [5.8204e-03, 3.3004e-04, 5.1576e-02, 9.5718e-01, 3.2322e-04, 2.4193e-04,\n",
      "         1.2388e-02, 2.6075e-04],\n",
      "        [9.9925e-01, 4.7228e-06, 2.1164e-04, 2.2263e-04, 1.1235e-05, 3.5593e-06,\n",
      "         2.2845e-04, 5.7309e-06]], grad_fn=<SelectBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 0.005846502259373665\n",
      "tensor([[9.9748e-01, 5.3653e-06, 4.3445e-04, 4.1679e-04, 5.2898e-06, 4.5013e-06,\n",
      "         4.0014e-04, 4.2688e-06],\n",
      "        [1.3640e-03, 1.6886e-04, 3.7504e-03, 9.8890e-01, 1.5453e-04, 1.2238e-04,\n",
      "         9.7097e-03, 9.8814e-05],\n",
      "        [9.9979e-01, 4.6479e-06, 1.0209e-04, 6.3476e-05, 3.7264e-06, 2.2395e-06,\n",
      "         1.7646e-04, 4.8113e-06],\n",
      "        [4.3834e-03, 1.0472e-04, 1.4302e-02, 9.0631e-03, 7.9779e-05, 4.5865e-05,\n",
      "         9.8922e-01, 9.4062e-05]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "Curr Loss: 0.002187940524891019\n",
      "tensor([[9.9985e-01, 3.3548e-06, 1.0479e-04, 6.7480e-05, 2.3639e-06, 1.2183e-06,\n",
      "         2.9921e-05, 4.3686e-06],\n",
      "        [9.9968e-01, 1.4708e-06, 2.4544e-05, 4.1795e-05, 2.0383e-06, 1.1414e-06,\n",
      "         3.2085e-04, 1.6884e-06],\n",
      "        [9.9972e-01, 3.4315e-06, 2.5273e-05, 1.0747e-04, 1.8736e-06, 1.8000e-06,\n",
      "         1.3241e-04, 3.9141e-06],\n",
      "        [9.9979e-01, 1.0346e-06, 4.2944e-05, 2.6359e-05, 9.6221e-07, 6.1381e-07,\n",
      "         5.5778e-05, 1.0586e-06]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 6.15400931565091e-05\n",
      "tensor([[1.6894e-04, 1.5315e-04, 1.1521e-02, 9.9006e-01, 1.1846e-04, 1.2650e-04,\n",
      "         5.7482e-03, 1.1095e-04],\n",
      "        [9.9980e-01, 4.2698e-07, 2.6218e-05, 2.7392e-05, 5.0529e-07, 3.1209e-07,\n",
      "         4.2054e-05, 3.9094e-07],\n",
      "        [9.9966e-01, 1.1532e-05, 1.8645e-04, 3.1929e-05, 5.1622e-06, 3.1525e-06,\n",
      "         1.3642e-04, 2.8315e-06],\n",
      "        [7.2021e-03, 8.3310e-05, 1.9438e-03, 3.2454e-03, 6.5152e-05, 3.5942e-05,\n",
      "         9.8945e-01, 5.8710e-05]], grad_fn=<SelectBackward0>) tensor([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0.]])\n",
      "Curr Loss: 0.0016345855547115207\n",
      "tensor([[9.9993e-01, 1.1326e-06, 1.7242e-05, 3.6655e-05, 7.5286e-07, 5.0490e-07,\n",
      "         8.1384e-05, 1.4893e-06],\n",
      "        [9.9990e-01, 1.2735e-06, 5.2366e-05, 4.6730e-05, 1.4056e-06, 7.7325e-07,\n",
      "         1.9577e-05, 1.4131e-06],\n",
      "        [9.9971e-01, 1.5911e-06, 1.0501e-04, 4.5263e-06, 1.4068e-06, 7.0979e-07,\n",
      "         1.0500e-04, 2.6097e-06],\n",
      "        [9.9980e-01, 1.6221e-06, 5.0464e-05, 3.3188e-05, 2.1982e-06, 1.2289e-06,\n",
      "         1.2073e-04, 1.4193e-06]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 4.27101222157944e-05\n",
      "tensor([[9.9997e-01, 7.4882e-07, 2.6107e-05, 9.5149e-06, 6.8090e-07, 4.3723e-07,\n",
      "         7.4557e-06, 4.5791e-07],\n",
      "        [9.9995e-01, 3.8505e-07, 7.8336e-06, 1.1488e-05, 3.7515e-07, 2.3755e-07,\n",
      "         3.7433e-05, 4.2465e-07],\n",
      "        [9.9985e-01, 1.1377e-06, 1.0106e-05, 5.8251e-05, 8.6557e-07, 3.8897e-07,\n",
      "         1.5344e-04, 7.1913e-07],\n",
      "        [9.9985e-01, 3.5784e-07, 3.8107e-05, 4.8799e-06, 4.0073e-07, 2.1622e-07,\n",
      "         6.5047e-05, 3.2847e-07]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 2.543648224673234e-05\n",
      "tensor([[9.9995e-01, 4.7660e-07, 1.7303e-05, 5.8524e-06, 4.0680e-07, 2.8126e-07,\n",
      "         2.5096e-05, 6.0038e-07],\n",
      "        [9.9998e-01, 1.2209e-06, 1.0331e-05, 2.8649e-05, 7.3750e-07, 7.3918e-07,\n",
      "         1.9191e-05, 1.0315e-06],\n",
      "        [9.9922e-01, 3.6998e-07, 1.2473e-05, 1.9207e-05, 2.5700e-07, 1.5349e-07,\n",
      "         3.9393e-05, 1.6893e-07],\n",
      "        [9.9999e-01, 2.8430e-07, 8.8631e-06, 1.9141e-06, 2.5723e-07, 9.0812e-08,\n",
      "         8.8948e-06, 1.6938e-07]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "Curr Loss: 3.2925254345173016e-05\n",
      "tensor([[9.9996e-01, 2.8125e-07, 9.4531e-06, 1.0155e-05, 3.2546e-07, 2.4762e-07,\n",
      "         8.8092e-06, 3.6691e-07],\n",
      "        [5.2486e-05, 2.1418e-05, 2.3565e-04, 9.9875e-01, 1.6693e-05, 1.6996e-05,\n",
      "         7.0893e-04, 1.6679e-05],\n",
      "        [9.9896e-01, 6.1556e-07, 4.1081e-05, 3.4673e-05, 1.0084e-06, 3.4571e-07,\n",
      "         3.3448e-05, 7.0331e-07],\n",
      "        [2.2060e-04, 1.7042e-05, 1.6109e-04, 9.9959e-01, 1.6034e-05, 1.3528e-05,\n",
      "         4.2667e-04, 1.2466e-05]], grad_fn=<SelectBackward0>) tensor([[1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0.]])\n",
      "Curr Loss: 0.0001507091656094417\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m     51\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 52\u001b[0m     train_step(e\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m, train_loader, batch_size)\n",
      "Cell \u001b[0;32mIn[107], line 30\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(epochs, dataloader, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[39mprint\u001b[39m(output, y)\n\u001b[1;32m     28\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCurr Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     31\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniforge3/envs/spotter/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/spotter/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fields for the model\n",
    "model = EmotionModel(\n",
    "    vocab_size= embedding_matrix_t.shape[0],\n",
    "    output_size= vectorized_labels_t.shape[1],\n",
    "    embedding_matrix= embedding_matrix_t,\n",
    "    hidden_dim= 128,\n",
    "    n_layers= 2,\n",
    "    drop_prob=0.2\n",
    "    ).to(device=device)\n",
    "loss_fn = nn.BCELoss().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "\n",
    "def train_step(epochs, dataloader, batch_size):\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size=batch_size, device=device)\n",
    "    \n",
    "    for i, (x, y) in enumerate(dataloader):\n",
    "        h = tuple([e.data for e in h])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        output, h = model(x, h)\n",
    "        loss = loss_fn(output, y.float())\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(output, y)\n",
    "            print(f\"Curr Loss: {loss.item()}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        # break\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 4\n",
    "\n",
    "# Balancing the data set\n",
    "from collections import Counter\n",
    "count = Counter(label_encoded)\n",
    "count = sorted([(k,v) for k,v in count.items()])\n",
    "count = np.array([c[1] for c in count])\n",
    "\n",
    "weights =  get_balance_weight(train_y)\n",
    "\n",
    "sampler = WeightedRandomSampler(weights, train_X.shape[0],replacement=True)\n",
    "\n",
    "train_data = TensorDataset(train_X, train_y)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "for e in range(epochs):\n",
    "    print(f\"Epoch: {e + 1}:\")\n",
    "    train_step(e+1, train_loader, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d484f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spotter",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "85873e3c2f23d47c926a81a8e94c0ccee54e84f329ff08cc51e05afe72c04afe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
